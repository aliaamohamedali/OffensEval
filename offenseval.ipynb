{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "## Feature Extraction\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "## Basic ML Models\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier \n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier, BaggingClassifier\n",
    "## Statistics\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "## Grid Search\n",
    "from sklearn.pipeline import Pipeline\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "## Text Preprocessing\n",
    "from gensim.parsing.preprocessing import split_alphanum\n",
    "from gensim.parsing.preprocessing import strip_numeric\n",
    "from gensim.parsing.preprocessing import strip_non_alphanum\n",
    "from gensim.parsing.preprocessing import strip_punctuation\n",
    "from gensim.parsing.preprocessing import strip_short\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "from gensim.parsing.preprocessing import stem_text\n",
    "from gensim.parsing.preprocessing import strip_multiple_whitespaces\n",
    "\n",
    "## Extras\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from gensim.models.wrappers import FastText\n",
    "#from wordsegment import load as ld, segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "## CharCNN & WordCNN\n",
    "from keras.layers import Input, Embedding, Conv1D, Convolution1D, MaxPooling1D, Dense\n",
    "from keras.layers import Flatten, Concatenate, Dropout, SpatialDropout1D, Reshape\n",
    "\n",
    "## RNN\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.callbacks import Callback\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "## Extras\n",
    "from keras.callbacks import TensorBoard\n",
    "from keras.callbacks import Callback, ModelCheckpoint, EarlyStopping\n",
    "from keras.models import load_model\n",
    "from keras.utils.vis_utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EDA\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data + embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_file = 'crawl-300d-2M.vec\\crawl-300d-2M.vec'\n",
    "aux_data_path = \"english/Data/agr_en_train.csv\"\n",
    "test_path = \"english/Test/agr_en_test.tfdif\"\n",
    "\n",
    "main_data_path = \"kl/offenseval-training-v1.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main = pd.read_csv(main_data_path, sep='\\t', header=0, usecols=[1, 2], names = ['text', 'label'])\n",
    "auxiliary = pd.read_csv(aux_data_path, delimiter=',', header=None, usecols=[1, 2], names = ['text', 'label'])\n",
    "corpus = main.append(auxiliary)\n",
    "corpus = corpus[corpus['label'] != \"CAG\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = corpus['text'].values\n",
    "labels = corpus['label'].map({'OAG': 1, 'NAG': 0, 'CAG': 0, 'OFF': 1, 'NOT': 0}).values\n",
    "text = text.reshape((text.size))\n",
    "labels = labels.reshape((labels.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coeffs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict = dict(get_coeffs(*line.rstrip().rsplit(' ')) for line in open(embedding_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_statistics(model):\n",
    "    train_pred = model.predict(X_train)\n",
    "    print(\"Training Data Score:\", accuracy_score(y_train, train_pred))\n",
    "    predictions = model.predict(X_test)\n",
    "    print(\"Test Data Score:\", accuracy_score(y_test, predictions))\n",
    "    print(\"Precision: \", precision_score(y_test, predictions))\n",
    "    print(\"Recall: \", recall_score(y_test, predictions))\n",
    "    print(\"F1 Score\", f1_score(y_test, predictions))\n",
    "\n",
    "    #confusion_matrix(y_test, predictions)\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, predictions).ravel()\n",
    "    print(tp, fp)\n",
    "    print(fn, tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned = text\n",
    "    cleaned = split_alphanum(cleaned.lower())\n",
    "    cleaned = strip_numeric(cleaned)\n",
    "    cleaned = strip_punctuation(cleaned)\n",
    "    cleaned = strip_short(cleaned, minsize=3)\n",
    "    cleaned = remove_stopwords(cleaned)\n",
    "    cleaned = stem_text(cleaned)\n",
    "    cleaned = re.sub('user', '', cleaned)\n",
    "    cleaned = re.sub('url', '', cleaned)\n",
    "    cleaned = strip_multiple_whitespaces(cleaned)\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = np.array([clean_text(sentence) for sentence in text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'text': text, 'clean_text': cleaned_text, 'labels': labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Get number of examples in each class:\n",
    "offensive = data[data['labels'] == 1]\n",
    "inoffensive = data[data['labels'] == 0]\n",
    "num_off = offensive.shape[0]\n",
    "num_inoff = inoffensive.shape[0]\n",
    "print(\"Offensive: \", num_off)\n",
    "print(\"Inoffensive: \", num_inoff)\n",
    "print('Offensive data Accounts for {} of total data'.format(num_off/(num_off+num_inoff)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### Basic Summary and statistics\n",
    "all_words = []\n",
    "all_literals = []\n",
    "for eg in data['text']:\n",
    "    for w in eg.split():\n",
    "        all_literals.extend(w)\n",
    "        all_words.append(w)  \n",
    "total = np.size(all_words)\n",
    "avg = np.size(all_words)/np.size(data['text'])\n",
    "unique_literals = len(set(all_literals))\n",
    "unique_words = len(set(all_words))\n",
    "\n",
    "all_words = []\n",
    "all_literals = []\n",
    "for eg in data['clean_text']:\n",
    "    for w in eg.split():\n",
    "        all_literals.extend(w)\n",
    "        all_words.append(w)        \n",
    "clean_total = np.size(all_words)\n",
    "clean_avg = np.size(all_words)/np.size(data['clean_text'])\n",
    "clean_unique_literals = len(set(all_literals))\n",
    "clean_unique_words = len(set(all_words))\n",
    "\n",
    "total = [total, clean_total]\n",
    "avg = [avg, clean_avg]\n",
    "unique_words = [unique_words, clean_unique_words]\n",
    "unique_literals = [unique_literals, clean_unique_literals]\n",
    "\n",
    "statistics_df = pd.DataFrame({'totals': total, 'avg': avg, 'unique_words': unique_words, 'unique_chars': unique_literals})\n",
    "statistics_df = statistics_df.rename(index={0: 'text', 1: 'clean_text'})\n",
    "statistics_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Top words:\n",
    "all_text = {}\n",
    "most_common = 20\n",
    "for sentence in text:\n",
    "    for word in sentence.split():\n",
    "        if word in all_text:\n",
    "            all_text[word] +=1\n",
    "        else:\n",
    "            all_text[word] = 1\n",
    "counts = Counter(all_text)\n",
    "most_common_before = pd.DataFrame(counts.most_common(most_common), columns = ['word', 'count'])\n",
    "\n",
    "all_text = {}\n",
    "for sentence in cleaned_text:\n",
    "    for word in sentence.split():\n",
    "        if word in all_text:\n",
    "            all_text[word] +=1\n",
    "        else:\n",
    "            all_text[word] = 1\n",
    "counts = Counter(all_text)\n",
    "most_common_after = pd.DataFrame(counts.most_common(most_common), columns = ['word_after', 'count_after'])\n",
    "\n",
    "all_text = {}\n",
    "for sentence in offensive['clean_text'].values:\n",
    "    for word in sentence.split():\n",
    "        if word in all_text:\n",
    "            all_text[word] += 1\n",
    "        else:\n",
    "            all_text[word] = 1\n",
    "counts = Counter(all_text)\n",
    "most_common_offens = pd.DataFrame(counts.most_common(most_common), columns = ['word_offens', 'count_offens'])                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "dfs = [most_common_before, most_common_after, most_common_offens]\n",
    "most_common_df = pd.concat(dfs, axis = 1)\n",
    "most_common_df.head()\n",
    "#most_common_df = most_common_df.drop(0)\n",
    "most_common_df.plot.bar(x='word', y='count')\n",
    "most_common_df.plot.bar(x='word_after', y='count_after')\n",
    "most_common_df.plot.bar(x='word_offens', y='count_offens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_wordcloud(data, title = None):\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        max_words=200,\n",
    "        max_font_size=40, \n",
    "        scale=3,\n",
    "        random_state=1 # chosen at random by flipping a coin; it was heads\n",
    "    ).generate(str(data))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(12, 12))\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=20)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_wordcloud(offensive['clean_text'])\n",
    "show_wordcloud(data['clean_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For charCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_char_example(text, max_chars= 300):\n",
    "    alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    categories = np.array(list(alphabet))\n",
    "    loc_dict = {k: i for k, i in zip(categories, np.arange(categories.size))}\n",
    "    \n",
    "    literals = np.array(list(text))\n",
    "    \n",
    "    encoding = np.zeros((max_chars, categories.size))\n",
    "    for i, literal in zip(np.arange(max_chars), literals):\n",
    "        if literal in loc_dict:\n",
    "            encoding[i, loc_dict[literal]] = 1    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_char_features(text, max_chars = 300):\n",
    "    features = np.array([encode_char_example(eg.lower()) for eg in cleaned_text])        \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For WordCNN & RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_word_features(text, use_embedding, embedding_dict, max_words = 50, embedding_length = 300):\n",
    "    features = np.zeros((text.shape[0], max_words, embedding_length))\n",
    "    if use_embedding:\n",
    "        j = 0\n",
    "        for eg in text:\n",
    "            i = 0\n",
    "            for word in eg.split(' '):\n",
    "                if i < max_words and word in embedding_dict:\n",
    "                    features[j, i, :] = embedding_dict[word]           \n",
    "    else:\n",
    "        ## Prepare tokenize and sequence (Check kaggle)   \n",
    "        tokenizer = Tokenizer(num_words = max_words)\n",
    "        tokenizer.fit_on_texts(text)\n",
    "        features = tokenizer.texts_to_sequences(text)\n",
    "        features = pad_sequences(features, maxlen = max_words)\n",
    "        \n",
    "    return features "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For ML Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ml_features(text, max_features = 10000, mode = 'BoW', analyzer = 'word', ngram_range = (1, 3)):\n",
    "    if mode == 'BoW-tfidf':\n",
    "        vect = CountVectorizer(analyzer = analyzer, ngram_range = ngram_range, max_features = max_features, max_df = 0.5)\n",
    "        tf_idf = TfidfTransformer()\n",
    "        features = vect.fit_transform(text)\n",
    "        features = tf_idf.fit_transform(features)\n",
    "    elif mode == 'tfidf':\n",
    "        tf_idf = TfidfVectorizer(analyzer = analyzer, ngram_range = ngram_range, max_features = max_features)\n",
    "        features = tf_idf.fit_transform(text.astype('U'))\n",
    "    else: # mode = 'Bow'\n",
    "        vect = CountVectorizer(analyzer = analyzer, ngram_range = ngram_range, max_features = max_features, max_df = 0.5)\n",
    "        features = vect.fit_transform(text)\n",
    "    return features    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(text, labels, mode = 'wordCNN', ml_feat = 'BoW', use_embedding = True, embedding_dict = None):\n",
    "    if mode == 'wordCNN':\n",
    "        return extract_word_features(text, use_embedding, embedding_dict)\n",
    "    if mode == 'charCNN':\n",
    "        return extract_char_features(text)\n",
    "    return extract_ml_features(text, mode = ml_feat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic ML Models - Parameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(cleaned_text, labels, mode = 'ml', use_embedding = False, embedding_dict = None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state = 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Multinominal Naive Bayes\n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "\n",
    "alpha_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "for alpha_val in alpha_values:\n",
    "    mNB = MultinomialNB(alpha=alpha_val)\n",
    "    mNB.fit(X_train, y_train)\n",
    "    predictions = mNB.predict(X_test)\n",
    "    test_precision_scores.append(precision_score(y_test, predictions))\n",
    "    test_recall_scores.append(recall_score(y_test, predictions))    \n",
    "    test_f1_scores.append(f1_score(y_test, predictions))\n",
    "    #print_statistics(mNB)\n",
    "\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores))    \n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Bernoulli Naive Bayes\n",
    "alpha_values = [0.001, 0.01, 0.1, 1, 10]\n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "for alpha_val in alpha_values:\n",
    "    bNB = BernoulliNB(alpha=alpha_val)\n",
    "    bNB.fit(X_train, y_train)\n",
    "    predictions = bNB.predict(X_test)\n",
    "    test_precision_scores.append(precision_score(y_test, predictions))\n",
    "    test_recall_scores.append(recall_score(y_test, predictions))  \n",
    "    test_f1_scores.append(f1_score(y_test, predictions))\n",
    "    #print_statistics(mNB)\n",
    "\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores)) \n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Suport Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SVM\n",
    "c_values = [0.01, 0.1, 1, 10]\n",
    "gamma_values = [0.001, 0.01, 0.1, 1]\n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "for c_val in c_values:\n",
    "    svm = SVC(C = c_val)\n",
    "    svm.fit(X_train, y_train)\n",
    "    predictions = svm.predict(X_test)\n",
    "    test_precision_scores.append(precision_score(y_test, predictions))\n",
    "    test_recall_scores.append(recall_score(y_test, predictions))\n",
    "    test_f1_scores.append(f1_score(y_test, predictions))\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores)) \n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regresision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### logistic regression\n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "C_values = [0.01, 0.1, 1, 10]\n",
    "max_it_vals = [50, 100, 200]\n",
    "for c_val in C_values:\n",
    "    for max_it_val in max_it_vals:\n",
    "        lr = LogisticRegression(C = c_val, max_iter = max_it_val)\n",
    "        lr.fit(X_train, y_train)\n",
    "        predictions = lr.predict(X_test)\n",
    "        test_precision_scores.append(precision_score(y_test, predictions))\n",
    "        test_recall_scores.append(recall_score(y_test, predictions))    \n",
    "        test_f1_scores.append(f1_score(y_test, predictions))\n",
    "        #print_statistics(mNB)\n",
    "\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores))\n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest\n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "      \n",
    "n_values = [5, 10, 20, 30, 40]\n",
    "for n_val in n_values:\n",
    "    rf = RandomForestClassifier(n_estimators = n_val)\n",
    "    rf.fit(X_train, y_train)\n",
    "    predictions = rf.predict(X_test)\n",
    "    test_precision_scores.append(precision_score(y_test, predictions))\n",
    "    test_recall_scores.append(recall_score(y_test, predictions))  \n",
    "    test_f1_scores.append(f1_score(y_test, predictions))  \n",
    "    #print_statistics(mNB)\n",
    "\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores))\n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  \n",
    "test_precision_scores = []\n",
    "test_recall_scores = []\n",
    "test_f1_scores = []\n",
    "losses = ['modified_huber', 'hinge']\n",
    "alpha_values = [0.00001, 0.0001, 0.001, 0.01, 0.1, 1]\n",
    "max_it_vals = [1, 2, 3, 4, 5, 10, 20, 40]\n",
    "for loss in losses:\n",
    "    for alpha in alpha_values:\n",
    "        for max_iter in max_it_vals:\n",
    "            sgd = SGDClassifier(loss= loss, alpha= alpha, max_iter= max_iter)\n",
    "            sgd.fit(X_train, y_train)\n",
    "            predictions = sgd.predict(X_test)\n",
    "            test_precision_scores.append(precision_score(y_test, predictions))\n",
    "            test_recall_scores.append(recall_score(y_test, predictions)) \n",
    "            test_f1_scores.append(f1_score(y_test, predictions))  \n",
    "\n",
    "print('Precisions: ' )\n",
    "print(np.array(test_precision_scores))\n",
    "print('Recall: ')\n",
    "print(np.array(test_recall_scores))\n",
    "print('f1: ')\n",
    "print(np.array(test_f1_scores)) \n",
    "print('__________________________________________________________________________________________')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "mNB = MultinomialNB(alpha = 0.001)\n",
    "bNB = BernoulliNB(alpha = 0.01)\n",
    "svm = LinearSVC(C = 1)\n",
    "lr = LogisticRegression(C = 10, max_iter = 50)\n",
    "rf = RandomForestClassifier(n_estimators = 30)\n",
    "sgd = SGDClassifier(max_iter = 4)\n",
    "xgb = XGBClassifier(max_depth=4, learning_rate=1.1, n_estimators=135)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.8457718212123274\n",
      "Test Data Score: 0.7812698412698412\n",
      "Precision:  0.7122040072859745\n",
      "Recall:  0.5647568608570053\n",
      "F1 Score 0.6299677765843179\n",
      "1173 474\n",
      "904 3749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "xgb.fit(X_train, y_train)\n",
    "print_statistics(xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_models = {'bNB': bNB, 'mnB': mNB, 'svm': svm, 'logisticRegression': lr, 'randomForest': rf, 'stochasticGD': sgd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.8514184638410777\n",
      "Test Data Score: 0.7463492063492063\n",
      "Precision:  0.6306601200218221\n",
      "Recall:  0.5565719788155994\n",
      "F1 Score 0.5913043478260869\n",
      "1156 677\n",
      "921 3546\n"
     ]
    }
   ],
   "source": [
    "bNB.fit(X_train, y_train)\n",
    "print_statistics(bNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.8582896795700388\n",
      "Test Data Score: 0.7353968253968254\n",
      "Precision:  0.5967894239848914\n",
      "Recall:  0.6085700529610014\n",
      "F1 Score 0.602622169249106\n",
      "1264 854\n",
      "813 3369\n"
     ]
    }
   ],
   "source": [
    "mNB.fit(X_train, y_train)\n",
    "print_statistics(mNB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9519695217361726\n",
      "Test Data Score: 0.7431746031746032\n",
      "Precision:  0.6184821889519876\n",
      "Recall:  0.5767934520943668\n",
      "F1 Score 0.5969108121574489\n",
      "1198 739\n",
      "879 3484\n"
     ]
    }
   ],
   "source": [
    "svm.fit(X_train, y_train)\n",
    "print_statistics(svm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9383631539560514\n",
      "Test Data Score: 0.7595238095238095\n",
      "Precision:  0.6533842794759825\n",
      "Recall:  0.5763119884448724\n",
      "F1 Score 0.612432847275518\n",
      "1197 635\n",
      "880 3588\n"
     ]
    }
   ],
   "source": [
    "lr.fit(X_train, y_train)\n",
    "print_statistics(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9910197972651201\n",
      "Test Data Score: 0.7641269841269841\n",
      "Precision:  0.6628099173553719\n",
      "Recall:  0.5792007703418391\n",
      "F1 Score 0.6181911613566289\n",
      "1203 612\n",
      "874 3611\n"
     ]
    }
   ],
   "source": [
    "rf.fit(X_train, y_train)\n",
    "print_statistics(rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.8601945710592558\n",
      "Test Data Score: 0.7515873015873016\n",
      "Precision:  0.6403508771929824\n",
      "Recall:  0.562349542609533\n",
      "F1 Score 0.5988208151755959\n",
      "1168 656\n",
      "909 3567\n"
     ]
    }
   ],
   "source": [
    "sgd.fit(X_train, y_train)\n",
    "print_statistics(sgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for name, model in basic_models.items():\n",
    "#    model.fit(X_train, y_train)\n",
    "#    print(\"Model: \", name)\n",
    "#    print_statistics(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_vote_model = VotingClassifier(estimators=[('MNB', mNB), ('BNB', bNB), ('SVM', svm), ('lr', lr), ('rf', rf), ('sgd',sgd), ('xgb', xgb)], voting='hard')\n",
    "max_vote_model = max_vote_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9285665691543642\n",
      "Test Data Score: 0.7741269841269841\n",
      "Precision:  0.6991473812423873\n",
      "Recall:  0.5527202696196437\n",
      "F1 Score 0.6173702608228019\n",
      "1148 494\n",
      "929 3729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "print_statistics(max_vote_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ML Ensemble Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stack Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-010346c9fcc0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mStackClassifier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, models = {'1': bNB, '2': mNB, '3': svm, '4': rf, '5': sgd, '6': xgb, 'bigModel': lr},\n\u001b[0;32m      3\u001b[0m                 mapping = {1: 'OAG', 0: 'NAG'}, n_fold = 8):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-010346c9fcc0>\u001b[0m in \u001b[0;36mStackClassifier\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStackClassifier\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     def __init__(self, models = {'1': bNB, '2': mNB, '3': svm, '4': rf, '5': sgd, '6': xgb, 'bigModel': lr},\n\u001b[0m\u001b[0;32m      3\u001b[0m                 mapping = {1: 'OAG', 0: 'NAG'}, n_fold = 8):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bNB' is not defined"
     ]
    }
   ],
   "source": [
    "class StackClassifier:\n",
    "    def __init__(self, models = {'1': bNB, '2': mNB, '3': svm, '4': rf, '5': sgd, '6': xgb, 'bigModel': lr},\n",
    "                mapping = {1: 'OAG', 0: 'NAG'}, n_fold = 8):\n",
    "        self.models = []\n",
    "        for key, value in models.items():\n",
    "            if key != 'bigModel':\n",
    "                self.models.append(value)\n",
    "            else:\n",
    "                self.big_model = value  \n",
    "        self.mapping = mapping\n",
    "        self.n_fold = n_fold\n",
    "        self.train_predictions = None\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.train_predictions = np.zeros((self.y_train.shape[0],1))\n",
    "        i = 0\n",
    "        for model in self.models:\n",
    "            i = i+1\n",
    "            model_train = self.stack(model)\n",
    "            self.train_predictions = np.concatenate((self.train_predictions, model_train), axis = 1)\n",
    "            print(i)\n",
    "            \n",
    "        self.big_model = self.big_model.fit(self.train_predictions, self.y_train)\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        level2_features = np.zeros((X_test.shape[0], 1))\n",
    "        for model in self.models:\n",
    "            model_pred = model.predict(X_test)\n",
    "            model_pred = model_pred.reshape(model_pred.size, 1)\n",
    "            level2_features = np.concatenate((level2_features, model_pred), axis = 1)\n",
    "        \n",
    "        return self.big_model.predict(level2_features)\n",
    "        \n",
    "    def stack(self, model):\n",
    "        \n",
    "        folds = StratifiedKFold(n_splits = self.n_fold, random_state=1)\n",
    "        \n",
    "        train_pred = np.empty((0,1), float)\n",
    "        \n",
    "        for train_indices, val_indices in folds.split(self.X_train, self.y_train):\n",
    "            x_train, x_val = self.X_train[train_indices], self.X_train[val_indices]\n",
    "            y_train, y_val = self.y_train[train_indices], self.y_train[val_indices]\n",
    "\n",
    "            model.fit(x_train, y_train)\n",
    "            \n",
    "            train_pred = np.append(train_pred, model.predict(x_val))\n",
    "            \n",
    "        return train_pred.reshape((train_pred.size, 1)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging + Maxvote Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bNB' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-5f5626fad334>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mclass\u001b[0m \u001b[0mBagging_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     def __init__(self, base_estimators = [bNB, mNB, svm, rf, lr, sgd, xgb],\n\u001b[0;32m      3\u001b[0m                 n_estimators = 8, mapping = {1: 'OAG', 0: 'NAG'}):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-5f5626fad334>\u001b[0m in \u001b[0;36mBagging_Classifier\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mBagging_Classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     def __init__(self, base_estimators = [bNB, mNB, svm, rf, lr, sgd, xgb],\n\u001b[0m\u001b[0;32m      3\u001b[0m                 n_estimators = 8, mapping = {1: 'OAG', 0: 'NAG'}):\n\u001b[0;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimators\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_estimators\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bNB' is not defined"
     ]
    }
   ],
   "source": [
    "class Bagging_Classifier():\n",
    "    def __init__(self, base_estimators = [bNB, mNB, svm, rf, lr, sgd, xgb],\n",
    "                n_estimators = 8, mapping = {1: 'OAG', 0: 'NAG'}):\n",
    "        self.base_estimators = np.array(base_estimators) \n",
    "        self.n_estimators = n_estimators\n",
    "        self.mapping = mapping\n",
    "        self.trained_models = []\n",
    "        self.X_train = None\n",
    "        self.y_train = None\n",
    "        \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        for model in self.base_estimators:\n",
    "            trained = BaggingClassifier(base_estimator = model, n_estimators = self.n_estimators)\n",
    "            trained = trained.fit(self.X_train, self.y_train)\n",
    "            self.trained_models.append(trained)\n",
    "            \n",
    "    def predict(self, X_test):\n",
    "        predictions = np.zeros((len(self.trained_models), X_test.shape[0]))\n",
    "        for i, model in zip(np.arange(len(self.trained_models)), self.trained_models):\n",
    "            predictions[i, :] = model.predict(X_test)\n",
    "   \n",
    "        final_predictions = np.sum(predictions, axis = 0) > len(self.trained_models)/2\n",
    "        final_predictions = final_predictions.astype(np.int64)\n",
    "        return final_predictions        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Classifiers "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CharCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharCNN():\n",
    "    def __init__(self):\n",
    "        self.max_chars = 300\n",
    "        self.embedding_size = 26\n",
    "        self.num_filters = [128, 128]\n",
    "        self.kernel_sizes = [4, 4]\n",
    "        self.pool_size = [3, 3]\n",
    "        self.fc_size = 512\n",
    "        inp = Input(shape=(self.max_chars, self.embedding_size))\n",
    "        x = SpatialDropout1D(0.4)(inp)\n",
    "        \n",
    "        conv_0 = Conv1D(self.num_filters[0], kernel_size=(self.kernel_sizes[0]), padding = 'valid',\n",
    "                                                                                kernel_initializer='normal',\n",
    "                                                                                kernel_regularizer= l2(1.0),\n",
    "                                                                                activation='relu')(x)\n",
    "        maxpool_0 = MaxPooling1D(pool_size=self.pool_size[0])(conv_0)\n",
    "        \n",
    "        conv_1 = Conv1D(self.num_filters[1], kernel_size=(self.kernel_sizes[1]), padding = 'valid',\n",
    "                                                                                kernel_initializer='normal',\n",
    "                                                                                kernel_regularizer= l2(1.0),\n",
    "                                                                                activation='relu')(maxpool_0)\n",
    "        maxpool_1 = MaxPooling1D(pool_size=self.pool_size[1])(conv_1)\n",
    "        # batch_0 = BatchNormalization(axis=3)(conv_0)\n",
    "        # batch_1 = BatchNormalization(axis=3)(conv_1)\n",
    "        \n",
    "        z = Flatten()(maxpool_1)\n",
    "        z = Dense(self.fc_size, kernel_initializer='normal',\n",
    "                                kernel_regularizer= l2(1.0),\n",
    "                                activation='relu')(z)\n",
    "        z = Dropout(0.4)(z)\n",
    "        \n",
    "        outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "        self.model = Model(inputs=inp, outputs=outp)\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics = ['accuracy'])\n",
    "        self.model.summary()\n",
    "    \n",
    "    def plot_model(self):\n",
    "        plot_model(self.model, \n",
    "           to_file='char-cnn.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'extract_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-2855c64a613a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_embedding\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtrain_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m42\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMyWordCNN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'extract_features' is not defined"
     ]
    }
   ],
   "source": [
    "features = extract_features(text, labels, use_embedding = False, embedding_dict = None)\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)\n",
    "\n",
    "my = MyWordCNN()\n",
    "my.model.fit(x=train_X, y=train_y,\n",
    "                           batch_size= 256,\n",
    "                           verbose=1, epochs= 3,\n",
    "                           validation_data=(val_X, val_y)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyWordCNN():\n",
    "    def __init__(self, max_words = 50, max_NB_words=20000, embedding_dim= 300, embedding_dict = None):\n",
    "        self.num_filters = [128, 128, 128]\n",
    "        self.kernel_sizes = [1, 2, 3]\n",
    "        self.pool_size = [3, 3, 3]\n",
    "        \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_words = max_words\n",
    "        self.max_NB_words = max_NB_words\n",
    "        if embedding_dict is None:\n",
    "            print('No dict passed')\n",
    "            self.embedding_matrix = np.random.random((self.max_NB_words, self.embedding_dim))\n",
    "            inp = Input(shape=(self.max_words, ))\n",
    "            x = Embedding(input_dim = self.max_NB_words, output_dim=self.embedding_dim, input_length=self.max_words, \n",
    "                  weights=[self.embedding_matrix], trainable=True)(inp)\n",
    "        else:\n",
    "            inp = Input(shape = (self.max_words, self.embedding_dim))\n",
    "            x = inp\n",
    "        \n",
    "        \n",
    "        x = SpatialDropout1D(0.3)(x)\n",
    "        x = Reshape((self.max_words, self.embedding_dim))(x)\n",
    "        \n",
    "        conv_0 = Conv1D(self.num_filters[0], kernel_size=(self.kernel_sizes[0],), padding = 'valid',\n",
    "                                                                                kernel_initializer='normal',\n",
    "                                                                                kernel_regularizer= l2(0.8),\n",
    "                                                                                activation='relu')(x)\n",
    "        maxpool_0 = MaxPooling1D(pool_size=(self.max_words - self.kernel_sizes[0] + 1))(conv_0)\n",
    "        flatten_0 = Flatten()(maxpool_0)\n",
    "        \n",
    "        conv_1 = Conv1D(self.num_filters[1], kernel_size=(self.kernel_sizes[1],), padding = 'valid',\n",
    "                                                                                kernel_initializer='normal',\n",
    "                                                                                kernel_regularizer= l2(0.8),\n",
    "                                                                                activation='relu')(x)\n",
    "        maxpool_1 = MaxPooling1D(pool_size=(self.max_words - self.kernel_sizes[1] + 1))(conv_1)\n",
    "        flatten_1 = Flatten()(maxpool_1)\n",
    "        \n",
    "        conv_2 = Conv1D(self.num_filters[2], kernel_size=(self.kernel_sizes[2],), padding = 'valid',\n",
    "                                                                                kernel_initializer='normal',\n",
    "                                                                                kernel_regularizer= l2(0.8),\n",
    "                                                                                activation='relu')(x)\n",
    "        maxpool_2 = MaxPooling1D(pool_size=(self.max_words - self.kernel_sizes[2] + 1))(conv_2)\n",
    "        flatten_2 = Flatten()(maxpool_2)\n",
    "        \n",
    "        conv_blocks = []\n",
    "        conv_blocks.append(flatten_0)\n",
    "        conv_blocks.append(flatten_1)\n",
    "        conv_blocks.append(flatten_2)\n",
    "        conc = Concatenate()(conv_blocks)\n",
    "        \n",
    "        #batch_0 = BatchNormalization(axis=3)(conv_0)\n",
    "        #batch_1 = BatchNormalization(axis=3)(conv_1)\n",
    "        #batch_2 = BatchNormalization(axis=3)(conv_2)\n",
    "         \n",
    "        z = Dropout(0.3)(conc)\n",
    "        \n",
    "        outp = Dense(1, activation=\"sigmoid\")(z)\n",
    "    \n",
    "        self.model = Model(inputs=inp, outputs=outp)\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                      metrics = ['accuracy'])\n",
    "        self.model.summary()\n",
    "        \n",
    "    def plot_model(self):\n",
    "        plot_model(self.model, \n",
    "           to_file='word-cnn.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCNN2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordCNN():\n",
    "    def __init__(\n",
    "            self, sequence_length, n_classes, vocab_size,\n",
    "            filter_sizes, num_filters,\n",
    "            embedding_size=300,\n",
    "            dropout_prob=0,\n",
    "            use_embedding_layer=True,\n",
    "            train_embedding=True,\n",
    "            embedding_matrix=None,\n",
    "            learning_rate=0.001):\n",
    "        \n",
    "        if use_embedding_layer:\n",
    "            inputs = Input(shape=(sequence_length,))\n",
    "\n",
    "            if embedding_matrix is not None:\n",
    "                embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                                            output_dim=embedding_size,\n",
    "                                            trainable=train_embedding,\n",
    "                                            weights=[embedding_matrix])(inputs)\n",
    "            else:\n",
    "                embedding_matrix = np.random.random((vocab_size, embedding_size))\n",
    "                embedding_layer = Embedding(input_dim=vocab_size,\n",
    "                                            output_dim=embedding_size,\n",
    "                                            trainable=train_embedding,\n",
    "                                            weights=[embedding_matrix])(inputs)\n",
    "        else:\n",
    "            inputs = Input(shape=(sequence_length, embedding_size))\n",
    "            embedding_layer = inputs\n",
    "\n",
    "        conv_blocks = []\n",
    "        for filter_size in filter_sizes:\n",
    "            conv = Convolution1D(filters=num_filters,\n",
    "                                 kernel_size=filter_size,\n",
    "                                 padding=\"valid\",\n",
    "                                 activation=\"relu\",\n",
    "                                 strides=1)(embedding_layer)\n",
    "            conv = MaxPooling1D(pool_size=sequence_length - filter_size + 1)(conv)\n",
    "            conv = Flatten()(conv)\n",
    "            conv_blocks.append(conv)\n",
    "\n",
    "        cnn = Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
    "\n",
    "        output = Dense(1,\n",
    "                            activation=\"sigmoid\")(Dropout(dropout_prob)(cnn))\n",
    "        self.model = Model(inputs, output)\n",
    "\n",
    "        self.model.compile(loss=\"binary_crossentropy\",\n",
    "                           optimizer=Adam(lr=learning_rate),\n",
    "                           metrics=[\"accuracy\"])\n",
    "        self.model.summary()\n",
    "        \n",
    "    def plot_model(self):\n",
    "        plot_model(self.model, \n",
    "           to_file='word-cnn.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, max_words = 50, embedding_dim = 300, max_NB_words=12000, use_embedding = True, with_conv = False):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_words = max_words\n",
    "        self.max_NB_words = max_NB_words\n",
    "        \n",
    "        if use_embedding:\n",
    "            inp = Input(shape=(self.max_words, self.embedding_dim))\n",
    "            x = inp\n",
    "        else:\n",
    "            self.embedding_matrix = np.random.random((self.max_NB_words, self.embedding_dim))\n",
    "            inp = Input(shape=(self.max_words, ))\n",
    "            x = Embedding(input_dim = self.max_NB_words, output_dim=self.embedding_dim, input_length=self.max_words, \n",
    "                  weights=[self.embedding_matrix], trainable=True)(inp)\n",
    "               \n",
    "           \n",
    "        x = SpatialDropout1D(0.3)(x)\n",
    "        x = Bidirectional(GRU(100, return_sequences=True))(x)\n",
    "        if with_conv:\n",
    "            x = Conv1D(64, kernel_size = 2, padding = \"same\", kernel_initializer = \"he_uniform\")(x)\n",
    "        avg_pool = GlobalAveragePooling1D()(x)\n",
    "        max_pool = GlobalMaxPooling1D()(x)\n",
    "        conc = Concatenate(axis = 1)([avg_pool, max_pool])\n",
    "        outp = Dense(1, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "        self.model = Model(inputs=inp, outputs=outp)\n",
    "        self.model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "        self.model.summary()\n",
    "    \n",
    "    def plot_model(self):\n",
    "        plot_model(self.model, \n",
    "           to_file='rnn.png', \n",
    "           show_shapes=True, \n",
    "           show_layer_names=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training The Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(cleaned_text, labels, mode = 'ml', ml_feat = 'BoW')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.3, random_state = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9152323287298456\n",
      "Test Data Score: 0.7790476190476191\n",
      "Precision:  0.7196921103271328\n",
      "Recall:  0.5402022147327876\n",
      "F1 Score 0.6171617161716171\n",
      "1122 437\n",
      "955 3786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "stack_model = StackClassifier()\n",
    "stack_model.fit(X_train, y_train)\n",
    "print_statistics(stack_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.9208789713585959\n",
      "Test Data Score: 0.775079365079365\n",
      "Precision:  0.7137305699481865\n",
      "Recall:  0.5305729417428984\n",
      "F1 Score 0.608671637669152\n",
      "1102 442\n",
      "975 3781\n"
     ]
    }
   ],
   "source": [
    "bagging_model = Bagging_Classifier()\n",
    "bagging_model.fit(X_train, y_train)\n",
    "print_statistics(bagging_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = 'training_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tb_callback = TensorBoard(log_dir=log_path, histogram_freq=0,\n",
    "                  write_graph=True, write_images=True)\n",
    "\n",
    "ckpt_callback = ModelCheckpoint(log_path + \"/weights.{epoch:02d}.hdf5\",\n",
    "                                monitor='val_acc', save_best_only=True,\n",
    "                                save_weights_only=False, mode='max', verbose=1)\n",
    "\n",
    "early_stop_callback = EarlyStopping(monitor='val_acc', min_delta=0, patience=1, verbose=0, mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(cleaned_text, labels, mode = 'charCNN', use_embedding = False, embedding_dict = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16799, 300, 26)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 300, 26)           0         \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 300, 26)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 297, 128)          13440     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 99, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 96, 128)           65664     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 32, 128)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 4096)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               2097664   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 2,177,281\n",
      "Trainable params: 2,177,281\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "char_cnn_model = CharCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13439 samples, validate on 3360 samples\n",
      "Epoch 1/2\n",
      " - 355s - loss: 2614.2288 - acc: 0.6550 - val_loss: 914.2241 - val_acc: 0.6565\n",
      "Epoch 2/2\n",
      " - 342s - loss: 442.5216 - acc: 0.6655 - val_loss: 163.3368 - val_acc: 0.6565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c0c94ba240>"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char_cnn_model.model.fit(x=train_X, y=train_y,\n",
    "                           batch_size= 256,\n",
    "                           verbose=2, epochs= 2,\n",
    "                           #callbacks=[tb_callback,\n",
    "                           #early_stop_callback, ckpt_callback],\n",
    "                           validation_data=(val_X, val_y)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_cnn_model.plot_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCNn & RNN with fastText embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(text, labels, use_embedding = True, embedding_dict = embedding_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 50, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 50, 128)      38528       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 49, 128)      76928       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 48, 128)      115328      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 128)          0           max_pooling1d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 128)          0           max_pooling1d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 128)          0           max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 384)          0           flatten_2[0][0]                  \n",
      "                                                                 flatten_3[0][0]                  \n",
      "                                                                 flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 1)            385         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 231,169\n",
      "Trainable params: 231,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_cnn_model = WordCNN(sequence_length = 50, \n",
    "                  n_classes = 2, \n",
    "                  vocab_size = 12000,\n",
    "                  filter_sizes = [1,2,3],\n",
    "                  num_filters = 128,\n",
    "                  use_embedding_layer = False,\n",
    "                  embedding_size=300,\n",
    "                  train_embedding = False,\n",
    "                  learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cnn_model.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13439 samples, validate on 3360 samples\n",
      "Epoch 1/3\n",
      " - 12s - loss: 0.6635 - acc: 0.6655 - val_loss: 0.6476 - val_acc: 0.6565\n",
      "Epoch 2/3\n",
      " - 12s - loss: 0.6393 - acc: 0.6655 - val_loss: 0.6433 - val_acc: 0.6565\n",
      "Epoch 3/3\n",
      " - 12s - loss: 0.6373 - acc: 0.6655 - val_loss: 0.6435 - val_acc: 0.6565\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c247af95c0>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_cnn_model.model.fit(x=train_X, y=train_y,\n",
    "                           batch_size= 256,\n",
    "                           verbose=2, epochs= 3,\n",
    "                           validation_data=(val_X, val_y)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            (None, 50, 300)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_2 (SpatialDro (None, 50, 300)      0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 50, 200)      240600      spatial_dropout1d_2[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 200)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 200)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 400)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 1)            401         concatenate_2[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 241,001\n",
      "Trainable params: 241,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model = RNN(use_embedding = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_model.plot_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13439 samples, validate on 3360 samples\n",
      "Epoch 1/7\n",
      " - 63s - loss: 0.6465 - acc: 0.6655 - val_loss: 0.6437 - val_acc: 0.6565\n",
      "Epoch 2/7\n",
      " - 23s - loss: 0.6381 - acc: 0.6655 - val_loss: 0.6444 - val_acc: 0.6565\n",
      "Epoch 3/7\n",
      " - 23s - loss: 0.6378 - acc: 0.6655 - val_loss: 0.6446 - val_acc: 0.6565\n",
      "Epoch 4/7\n",
      " - 24s - loss: 0.6378 - acc: 0.6655 - val_loss: 0.6433 - val_acc: 0.6565\n",
      "Epoch 5/7\n",
      " - 24s - loss: 0.6378 - acc: 0.6655 - val_loss: 0.6435 - val_acc: 0.6565\n",
      "Epoch 6/7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-24006141f46d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m                            \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m256\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m                            \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m                    )\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rnn_model.model.fit(x=train_X, y=train_y,\n",
    "                           batch_size= 256,\n",
    "                           verbose=2, epochs= 7,\n",
    "                           validation_data=(val_X, val_y)\n",
    "                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCNN & RNN training their own embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = extract_features(text, labels, use_embedding = False, embedding_dict = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size = 0.2, random_state = 42)\n",
    "train_X, val_X, train_y, val_y = train_test_split(X_train, y_train, test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 50, 300)      6000000     input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_3 (SpatialDro (None, 50, 300)      0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 50, 200)      240600      spatial_dropout1d_3[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_2 (Glo (None, 200)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 200)          0           bidirectional_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 400)          0           global_average_pooling1d_2[0][0] \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 1)            401         concatenate_3[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,241,001\n",
      "Trainable params: 6,241,001\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "rnn_model2 = RNN(use_embedding = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13439 samples, validate on 3360 samples\n",
      "Epoch 1/10\n",
      " - 35s - loss: 0.6573 - acc: 0.6479 - val_loss: 0.6322 - val_acc: 0.6571\n",
      "Epoch 2/10\n",
      " - 32s - loss: 0.6314 - acc: 0.6647 - val_loss: 0.6373 - val_acc: 0.6571\n",
      "Epoch 3/10\n",
      " - 31s - loss: 0.6262 - acc: 0.6642 - val_loss: 0.6305 - val_acc: 0.6583\n",
      "Epoch 4/10\n",
      " - 32s - loss: 0.6251 - acc: 0.6663 - val_loss: 0.6273 - val_acc: 0.6622\n",
      "Epoch 5/10\n",
      " - 32s - loss: 0.6226 - acc: 0.6685 - val_loss: 0.6271 - val_acc: 0.6592\n",
      "Epoch 6/10\n",
      " - 32s - loss: 0.6191 - acc: 0.6678 - val_loss: 0.6272 - val_acc: 0.6634\n",
      "Epoch 7/10\n",
      " - 33s - loss: 0.6172 - acc: 0.6730 - val_loss: 0.6551 - val_acc: 0.6560\n",
      "Epoch 8/10\n",
      " - 32s - loss: 0.6167 - acc: 0.6701 - val_loss: 0.6286 - val_acc: 0.6592\n",
      "Epoch 9/10\n",
      " - 32s - loss: 0.6193 - acc: 0.6672 - val_loss: 0.6269 - val_acc: 0.6610\n",
      "Epoch 10/10\n",
      " - 32s - loss: 0.6130 - acc: 0.6710 - val_loss: 0.6283 - val_acc: 0.6619\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c1b245a0b8>"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn_model2.model.fit(x=train_X, y=train_y,\n",
    "                    batch_size= 256,\n",
    "                    verbose=2, epochs= 10,\n",
    "                    validation_data=(val_X, val_y)\n",
    "                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            (None, 50)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 50, 300)      3600000     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 50, 128)      38528       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 49, 128)      76928       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 48, 128)      115328      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 128)          0           max_pooling1d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)             (None, 128)          0           max_pooling1d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_7 (Flatten)             (None, 128)          0           max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 384)          0           flatten_5[0][0]                  \n",
      "                                                                 flatten_6[0][0]                  \n",
      "                                                                 flatten_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 384)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 1)            385         dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,831,169\n",
      "Trainable params: 3,831,169\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "word_cnn_model2 = WordCNN(sequence_length = 50,\n",
    "                  n_classes = 2, \n",
    "                  vocab_size = 12000,\n",
    "                  filter_sizes = [1,2,3],\n",
    "                  num_filters = 128,\n",
    "                  use_embedding_layer = True,\n",
    "                  embedding_size=300,\n",
    "                  train_embedding = True,\n",
    "                  learning_rate = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 13439 samples, validate on 3360 samples\n",
      "Epoch 1/5\n",
      " - 990s - loss: 5.2984 - acc: 0.6602 - val_loss: 5.5358 - val_acc: 0.6565\n",
      "Epoch 2/5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-06e4916a3d2c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m                            \u001b[1;31m#callbacks=[tb_callback,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                            \u001b[1;31m#early_stop_callback, ckpt_callback],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m                            \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m                         )\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Users\\AbdElRahman\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "word_cnn_model2.model.fit(x=train_X, y=train_y,\n",
    "                           batch_size= 256,\n",
    "                           verbose=2, epochs= 5,\n",
    "                           #callbacks=[tb_callback,\n",
    "                           #early_stop_callback, ckpt_callback],\n",
    "                           validation_data=(val_X, val_y)\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests w kda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(X_train, y_train, train_size=0.8, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)\n",
    "\n",
    "model = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=2)\n",
    "\n",
    "\n",
    "y_pred = model.predict(x_test, batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[1])\n",
    "cleaned = split_alphanum(text[1].lower())\n",
    "print(cleaned)\n",
    "cleaned = strip_numeric(cleaned)\n",
    "print(cleaned)\n",
    "cleaned = strip_punctuation(cleaned)\n",
    "print(cleaned)\n",
    "cleaned = strip_short(cleaned, minsize=3)\n",
    "print(cleaned)\n",
    "cleaned = remove_stopwords(cleaned)\n",
    "print(cleaned)\n",
    "cleaned = stem_text(cleaned)\n",
    "print(cleaned)\n",
    "cleaned = re.sub('user', '', cleaned)\n",
    "cleaned = re.sub('url', '', cleaned)\n",
    "print(cleaned)\n",
    "cleaned = strip_multiple_whitespaces(cleaned)\n",
    "print(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect', CountVectorizer(analyzer='char', ngram_range= (2, 4), max_features=10000)),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('clf', SGDClassifier(max_iter = 5))])\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'vect__max_features': (None, 8000, 10000),\n",
    "    # 'vect__ngram_range': ((2, 4)),  # unigrams or bigrams\n",
    "    # 'tfidf__use_idf': (True, False),\n",
    "    # 'tfidf__norm': ('l1', 'l2'),\n",
    "    # 'clf__max_iter': (5,),\n",
    "    'clf__alpha': (0.05, 0.1, 0.5),\n",
    "    'clf__penalty': ('l2', 'elasticnet'),\n",
    "    # 'clf__max_iter': (10, 50, 80),\n",
    "}\n",
    "grid_search = GridSearchCV(pipeline, parameters, cv=5,\n",
    "                               n_jobs=-1, verbose=1)\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(text, labels)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Best score: %0.3f\" % grid_search.best_score_)\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = SVC()\n",
    "param_grid = [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "grid = GridSearchCV(estimator, param_grid = param_grid, cv=5)\n",
    "grid.fit(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-41-f57f5d273163>, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-41-f57f5d273163>\"\u001b[1;36m, line \u001b[1;32m17\u001b[0m\n\u001b[1;33m    'bag': bagging_model\u001b[0m\n\u001b[1;37m        ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "statistics = { # Aka: columns\n",
    "    'Train_Accuracy': [],\n",
    "    'Test_Accuracy': [],\n",
    "    'Precision': [],\n",
    "    'Recall': [],\n",
    "    'F1_score': [],\n",
    "}\n",
    "all_models = { # aka rows\n",
    "    'bNB': bNB,\n",
    "    'mNB': mNB,\n",
    "    'svm': svm,\n",
    "    'lr': lr,\n",
    "    'rf': rf,\n",
    "    'sgd': sgd,\n",
    "    'xgb': xgb,\n",
    "    'stack': stack_model\n",
    "    'bag': bagging_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
